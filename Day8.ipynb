{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOyEiLKSEy69y0MJS1tdlu4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGJg0QcyXTLm"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd\n",
        "\n",
        "datastore = pd.read_json('Sarcasm_Headline_Dataset.json', lines = True)\n",
        "\n",
        "sentences = datastore['headline'].tolist()\n",
        "labels = datastore['is_sarcastic'].tolist()\n",
        "urls = datastore['article_link'].tolist()\n",
        "\n",
        "#shuffle data to split into train, test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences, labels, train_size = 0.8, random_state = 42, shuffle = True)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "df = pd.read_json('Sarcasm_Headlines_Dataset.json', lines = True)\n",
        "\n",
        "df.head()\n",
        "\n",
        "#tokenize text\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 100\n",
        "training_size = 20000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "training_padded = pad_sequences(training_sequences,maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}